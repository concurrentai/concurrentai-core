# model-executor

Executes the actual inference request for each model and forwards the response.

Currently supported:
- MLflow Docker images

Anticipated future support:
- MLflow projects
- Kubeflow
- Custom Python
- ...


*See the [Contributing Guide](../../CONTRIBUTING.md) to get started building and testing code.*